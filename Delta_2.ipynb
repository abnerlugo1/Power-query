{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abnerlugo1/Power-query/blob/main/Delta_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIXCRdVjRTOT",
        "outputId": "f71db9d5-b357-4645-bed0-56d70de3a9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Tratamiento de datos\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Gráficos\n",
        "# ==============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#style.use('ggplot') or plt.style.use('ggplot')\n",
        "\n",
        "# Preprocesado y modelado\n",
        "# ==============================================================================\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración warnings\n",
        "# ==============================================================================\n",
        "#import libarchive\n",
        "#import pydot\n",
        "#import cartopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtvsFZdLToxM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "b2cd4332-2257-43aa-d781-5daf883e986e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/base_mejora_enero.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-647655418.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/base_mejora_enero.xlsx'\u001b[0m  \u001b[0;31m# base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/base_mejora_enero.xlsx'"
          ]
        }
      ],
      "source": [
        "# Lectura de datos\n",
        "# ==============================================================================\n",
        "file_path = '/content/base_mejora_enero.xlsx'  # base\n",
        "df7 = pd.read_excel(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lFzj51VsBV8"
      },
      "outputs": [],
      "source": [
        "df7.loc[df7[\"NPS\"].isin([10, 9]), 'result'] = \"promotor\"\n",
        "df7.loc[df7[\"NPS\"].isin([7, 8]), 'result'] = \"pasivo\"\n",
        "df7.loc[df7[\"NPS\"].isin([1, 2, 3, 4, 5, 6]), 'result'] = \"detractor\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3zh6k43sEnf"
      },
      "outputs": [],
      "source": [
        "df7.loc[df7[\"EDS_F\"].isin([5, 4]), 'result_EDS'] = \"promotor\"\n",
        "df7.loc[df7[\"EDS_F\"].isin([3]), 'result_EDS'] = \"pasivo\"\n",
        "df7.loc[df7[\"EDS_F\"].isin([1, 2]), 'result_EDS'] = \"detractor\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JG0WgbbNYJaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8PFwO5UsGpG"
      },
      "outputs": [],
      "source": [
        "df7.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VO5tE-HsNxz"
      },
      "outputs": [],
      "source": [
        "print(df7['Mejora'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzesJlrisegT"
      },
      "outputs": [],
      "source": [
        "print(df7.groupby('NPS').size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr_v3p08s3nM"
      },
      "outputs": [],
      "source": [
        "# Distribución temporal\n",
        "# ==============================================================================\n",
        "fig, ax = plt.subplots(figsize=(9,4))\n",
        "\n",
        "for NPS in df7.NPS.unique():\n",
        "    df_temp = df7[df7['NPS'] == NPS].copy()\n",
        "    df_temp['fecha_servicio'] = pd.to_datetime(df_temp['fecha_servicio'])\n",
        "    df_temp = df_temp.groupby(df_temp['fecha_servicio']).size()\n",
        "    df_temp.plot(label=NPS, ax=ax)\n",
        "\n",
        "ax.set_title('NPS por mes')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wc5E6_gtI5L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def limpiar_tokenizar(texto):\n",
        "    '''\n",
        "    Esta función limpia y tokeniza el texto en palabras individuales.\n",
        "    El orden en el que se va limpiando el texto no es arbitrario.\n",
        "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
        "    y re.escape(string.punctuation)\n",
        "    '''\n",
        "\n",
        "    # Se convierte todo el texto a minúsculas\n",
        "    nuevo_texto = texto.lower()\n",
        "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
        "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
        "    # Eliminación de signos de puntuación\n",
        "    regex = '[\\\\!\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^\\\\_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
        "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
        "    # Eliminación de números\n",
        "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
        "    # Eliminación de espacios en blanco múltiples\n",
        "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
        "    # Tokenización por palabras individuales\n",
        "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
        "    # Eliminación de tokens con una longitud < 2\n",
        "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
        "\n",
        "    return(nuevo_texto)\n",
        "\n",
        "test = \"Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
        "test1= \"12\"\n",
        "print(test)\n",
        "print(limpiar_tokenizar(texto=test1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1EGh8jWtVz_"
      },
      "outputs": [],
      "source": [
        "# Se aplica la función de quitar NAN\n",
        "# ==============================================================================\n",
        "cleaned_df = df7.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yX_UVJAtZ9n"
      },
      "outputs": [],
      "source": [
        "# Se aplica la función de limpieza y tokenización\n",
        "# ==============================================================================\n",
        "df7['texto_tokenizado'] = df7['Mejora'].fillna('').apply(lambda x: limpiar_tokenizar(x))\n",
        "df7[['Mejora', 'texto_tokenizado']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBs3c1Rrtjhc"
      },
      "outputs": [],
      "source": [
        "# Unnest de la columna texto_tokenizado\n",
        "# ==============================================================================\n",
        "tweets_tidy = df7.explode(column='texto_tokenizado')\n",
        "# Drop the original 'Mejora' column as its content is now in 'texto_tokenizado' (soon to be 'token')\n",
        "tweets_tidy = tweets_tidy.drop(columns='Mejora')\n",
        "# Rename the exploded 'texto_tokenizado' column to 'token'\n",
        "tweets_tidy = tweets_tidy.rename(columns={'texto_tokenizado':'token'})\n",
        "tweets_tidy.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADQU4k9FtnLk"
      },
      "outputs": [],
      "source": [
        "# Palabras totales utilizadas por segmento\n",
        "# ==============================================================================\n",
        "print('--------------------------')\n",
        "print('Palabras totales por servicio')\n",
        "print('--------------------------')\n",
        "tweets_tidy.groupby(by='EDS')['token'].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB9L56RYtuI5"
      },
      "outputs": [],
      "source": [
        "# Longitud media y desviación de los coementarios de cada autor\n",
        "# ==============================================================================\n",
        "temp_df = pd.DataFrame(tweets_tidy.groupby(by = [\"EDS_F\", \"token\"])[\"token\"].count())\n",
        "temp_df = temp_df.rename(columns={'token': 'token_count'}) # Rename the count column\n",
        "temp_df.reset_index().groupby(\"EDS_F\")[\"token_count\"].agg(['mean', 'std'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUzgO2SEt3dy"
      },
      "outputs": [],
      "source": [
        "# Top 5 palabras más utilizadas por edad_cat\n",
        "# ==============================================================================\n",
        "tweets_tidy.groupby(['EDS'])['token'] \\\n",
        " .count() \\\n",
        " .reset_index(name='count') \\\n",
        " .groupby('EDS') \\\n",
        " .apply(lambda x: x.sort_values('count', ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbTM2qC2t5pa"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "# Obtención de listado de stopwords del español\n",
        "# ==============================================================================\n",
        "stop_words = list(stopwords.words('spanish'))\n",
        "# Se añade la stoprword: amp, ax, ex\n",
        "stop_words.extend((\"por\", \"de\", \"para\", \"si\", \"vas\", \"mucha\", \"muchas\"\n",
        ",\"merece\", \"pudiera\",\"seco\",\"viaje\", \"ahi\", \"ahí\", \"normal\", \"ir\", \"acelera\"\n",
        ", \"además\", \"correo\", \"folio\", \"nunca\", \"buen\", \"bien\", \"buena\", \"sola\"\n",
        ", \"solo\", \"bueno\", \"ser\",\"uds\", \"vez\", \"dar\", \"así\", \"dio\", \"iban\", \"pedir\"\n",
        ",\"cambio\" , \"cuito\"  , \"jajaja\", \"literla\", \"mandarme\", \"masculino\", \"memencanto\"\n",
        ", \"mi\", \"dentro\", \"uds\", \"sé\", \"dónde\", \"dije\", \"el\"))\n",
        "print(stop_words[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWaUIeT_uYcF"
      },
      "outputs": [],
      "source": [
        "# Filtrado para excluir stopwords\n",
        "# ==============================================================================\n",
        "tweets_tidy = tweets_tidy[~(tweets_tidy[\"token\"].isin(stop_words))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxvtSjHNubej"
      },
      "outputs": [],
      "source": [
        "# Top 10 palabras por autor (sin stopwords)\n",
        "# ==============================================================================\n",
        "fig, axs = plt.subplots(nrows=5, ncols=1,figsize=(6, 8))\n",
        "for i, EDS in enumerate(tweets_tidy.EDS.unique()):\n",
        "    df_temp = tweets_tidy[tweets_tidy.EDS == EDS]\n",
        "    counts  = df_temp['token'].value_counts(ascending=False).head(10)\n",
        "    counts.plot(kind='barh', color='firebrick', ax=axs[i])\n",
        "    axs[i].invert_yaxis()\n",
        "    axs[i].set_title(EDS)\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEQwCUc0vWGs"
      },
      "outputs": [],
      "source": [
        "# Pivotado de datos\n",
        "# ==============================================================================\n",
        "tweets_pivot = tweets_tidy.groupby([\"EDS\",\"token\"])[\"token\"] \\\n",
        "                .agg([\"count\"]).reset_index() \\\n",
        "                .pivot(index = \"token\" , columns=\"EDS\", values= \"count\")\n",
        "tweets_pivot.columns.name = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFcX5BymvfB7"
      },
      "outputs": [],
      "source": [
        "# Test de correlación (coseno) por el uso y frecuencia de palabras\n",
        "# ==============================================================================\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def similitud_coseno(a,b):\n",
        "    distancia = cosine(a,b)\n",
        "    return 1-distancia\n",
        "\n",
        "tweets_pivot.corr(method=similitud_coseno)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q7JUsxRvnng"
      },
      "outputs": [],
      "source": [
        "# Gráfico de correlación\n",
        "# ==============================================================================\n",
        "f, ax = plt.subplots(figsize=(6, 4))\n",
        "temp = tweets_pivot.dropna()\n",
        "sns.regplot(\n",
        "    x  = np.log(temp['Puede mejorar']),\n",
        "    y  = np.log(temp['Cumple lo esperado']),\n",
        "    scatter_kws =  {'alpha': 0.05},\n",
        "    ax = ax\n",
        ");\n",
        "for i in np.random.choice(range(temp.shape[0]), 100):\n",
        "    ax.annotate(\n",
        "        text  = temp.index[i],\n",
        "        xy    = (np.log(temp['Puede mejorar'][i]), np.log(temp['Cumple lo esperado'][i])),\n",
        "        alpha = 0.7\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs9VLCXAwrEd"
      },
      "outputs": [],
      "source": [
        "# Número de palabras comunes\n",
        "# ==============================================================================\n",
        "palabras_promotor = set(tweets_tidy[tweets_tidy.EDS == 'Puede mejorar']['token'])\n",
        "palabras_pasivo = set(tweets_tidy[tweets_tidy.EDS == 'Insatisfecho']['token'])\n",
        "palabras_detractor = set(tweets_tidy[tweets_tidy.EDS == 'Supero expectativas']['token'])\n",
        "\n",
        "print(f\"Palabras comunes entre madrugada y tarde: {len(palabras_promotor.intersection(palabras_pasivo))}\")\n",
        "print(f\"Palabras comunes entre madrugada y mañana: {len(palabras_detractor.intersection(palabras_pasivo))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9rAqkZSw2hf"
      },
      "outputs": [],
      "source": [
        "# Cálculo del log of odds ratio de cada palabra (pasivo vs detractor)\n",
        "# ==============================================================================\n",
        "# Pivotaje y despivotaje\n",
        "tweets_pivot = tweets_tidy.groupby([\"servicio\",\"token\"])[\"token\"] \\\n",
        "                .agg([\"count\"]).reset_index() \\\n",
        "                .pivot(index = \"token\" , columns=\"EDS\", values= \"count\")\n",
        "\n",
        "tweets_pivot = tweets_pivot.fillna(value=0)\n",
        "tweets_pivot.columns.name = None\n",
        "\n",
        "tweets_unpivot = tweets_pivot.melt(value_name='n', var_name='EDS', ignore_index=False)\n",
        "tweets_unpivot = tweets_unpivot.reset_index()\n",
        "\n",
        "# Selección\n",
        "tweets_unpivot = tweets_unpivot[tweets_unpivot.EDS.isin(['Insatisfecho', 'Supero expectativas'])]\n",
        "\n",
        "# Se añade el total de palabras de cada autor\n",
        "tweets_unpivot = tweets_unpivot.merge(\n",
        "                    tweets_tidy.groupby('servicio')['token'].count().rename('N'),\n",
        "                    how = 'left',\n",
        "                    on  = 'servicio'\n",
        "                 )\n",
        "\n",
        "# Cálculo de odds y log of odds de cada palabra\n",
        "tweets_logOdds = tweets_unpivot.copy()\n",
        "tweets_logOdds['odds'] = (tweets_logOdds.n + 1) / (tweets_logOdds.N + 1)\n",
        "tweets_logOdds = tweets_logOdds[['token', 'servicio', 'odds']] \\\n",
        "                    .pivot(index='token', columns='servicio', values='odds')\n",
        "tweets_logOdds.columns.name = None\n",
        "\n",
        "tweets_logOdds['log_odds']     = np.log(tweets_logOdds['Supero expectativas']/tweets_logOdds['Insatisfecho'])\n",
        "tweets_logOdds['abs_log_odds'] = np.abs(tweets_logOdds.log_odds)\n",
        "\n",
        "# Si el logaritmo de odds es mayor que cero, significa que es una palabra con\n",
        "tweets_logOdds['autor_frecuente'] = np.where(tweets_logOdds.log_odds > 0,\n",
        "                                              \"Supero expectativas\",\n",
        "                                              \"Insatisfecho\"\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AoL2OgQxChw"
      },
      "outputs": [],
      "source": [
        "print('-----------------------------------')\n",
        "print('Top 10 palabras más diferenciadoras')\n",
        "print('-----------------------------------')\n",
        "tweets_logOdds.sort_values('abs_log_odds', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw-29mDCyyT0"
      },
      "outputs": [],
      "source": [
        "# Top 15 palabras más características de cada autor\n",
        "# ==============================================================================\n",
        "\n",
        "top_30 = tweets_logOdds[['log_odds', 'abs_log_odds', 'autor_frecuente']] \\\n",
        "        .groupby('autor_frecuente') \\\n",
        "        .apply(lambda x: x.nlargest(15, columns='abs_log_odds').reset_index()) \\\n",
        "        .reset_index(drop=True) \\\n",
        "        .sort_values('log_odds')\n",
        "\n",
        "f, ax = plt.subplots(figsize=(4, 7))\n",
        "sns.barplot(\n",
        "    x    = 'log_odds',\n",
        "    y    = 'token',\n",
        "    hue  = 'autor_frecuente',\n",
        "    data = top_30,\n",
        "    ax   = ax\n",
        ")\n",
        "ax.set_title('Top 15 palabras más características ')\n",
        "ax.set_xlabel('log odds ratio (50  / _<28)');\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cttwQY_z2pRk"
      },
      "outputs": [],
      "source": [
        "# Cálculo term-frecuency (tf)\n",
        "# ==============================================================================\n",
        "tf = tweets_tidy.copy()\n",
        "# Número de veces que aparece cada término\n",
        "tf = tf.groupby([\"id\", \"token\"])[\"token\"].agg([\"count\"]).reset_index()\n",
        "# Se añade una columna con el total de términos\n",
        "tf['total_count'] = tf.groupby('id')['count'].transform(sum)\n",
        "# Se calcula el tf\n",
        "tf['tf'] = tf[\"count\"] / tf[\"total_count\"]\n",
        "tf.sort_values(by = \"tf\").head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVG16lg-31TB"
      },
      "outputs": [],
      "source": [
        "# Inverse document frequency\n",
        "# ==============================================================================\n",
        "idf = tweets_tidy.copy()\n",
        "total_documents = idf[\"id\"].drop_duplicates().count()\n",
        "# Número de documentos (tweets) en los que aparece cada término\n",
        "idf = idf.groupby([\"token\", \"id\"])[\"token\"].agg([\"count\"]).reset_index()\n",
        "idf['n_documentos'] = idf.groupby('token')['count'].transform(sum)\n",
        "# Cálculo del idf\n",
        "idf['idf'] = np.log(total_documents / idf['n_documentos'])\n",
        "idf = idf[[\"token\",\"n_documentos\", \"idf\"]].drop_duplicates()\n",
        "idf.sort_values(by=\"idf\").head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNYkw9OL4KBy"
      },
      "outputs": [],
      "source": [
        "# Term Frequency - Inverse Document Frequency\n",
        "# ==============================================================================\n",
        "tf_idf = pd.merge(left=tf, right=idf, on=\"token\")\n",
        "tf_idf[\"tf_idf\"] = tf_idf[\"tf\"] * tf_idf[\"idf\"]\n",
        "tf_idf.sort_values(by=\"id\").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMTZVs6m4dQG"
      },
      "outputs": [],
      "source": [
        "# Reparto train y test\n",
        "# ==============================================================================\n",
        "# Filter tweets_tidy to only include the categories intended for classification\n",
        "# Assuming the user wants to classify 'pasivo' vs 'detractor'\n",
        "df_filtered = tweets_tidy[tweets_tidy.edad_cat.isin(['Edad_<_25', 'Edad_>_50'])].copy()\n",
        "\n",
        "# X should be the features (e.g., tokens) and y should be the target labels\n",
        "# Note: 'token' here is a single word from the exploded DataFrame. For actual classification,\n",
        "# these tokens would usually be vectorized (e.g., TF-IDF) per document/comment ID.\n",
        "datos_X = df_filtered['token']\n",
        "datos_y = df_filtered['edad_cat']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    datos_X,\n",
        "    datos_y,\n",
        "    test_size = 0.2,\n",
        "    random_state = 123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocLPsgiv4hSR"
      },
      "outputs": [],
      "source": [
        "value, counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(value, 100 * counts / sum(counts))))\n",
        "value, counts = np.unique(y_test, return_counts=True)\n",
        "print(dict(zip(value, 100 * counts / sum(counts))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gqkweiI4uOv"
      },
      "outputs": [],
      "source": [
        "# Creación de la matriz tf-idf\n",
        "# ==============================================================================\n",
        "tfidf_vectorizador = TfidfVectorizer(\n",
        "                        tokenizer  = limpiar_tokenizar,\n",
        "                        min_df     = 3,\n",
        "                        stop_words = stop_words\n",
        "                    )\n",
        "tfidf_vectorizador.fit(X_train.dropna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4vSxHWs4xHN"
      },
      "outputs": [],
      "source": [
        "tfidf_train = tfidf_vectorizador.transform(X_train.dropna())\n",
        "tfidf_test  = tfidf_vectorizador.transform(X_test.dropna())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghdIviXM45yt"
      },
      "outputs": [],
      "source": [
        "print(f\" Número de tokens creados: {len(tfidf_vectorizador.get_feature_names_out())}\")\n",
        "print(tfidf_vectorizador.get_feature_names_out()[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcuuvNyO5AzV"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo SVM\n",
        "# ==============================================================================\n",
        "modelo_svm_lineal = svm.SVC(kernel= \"linear\", C = 1.0)\n",
        "\n",
        "# Filter y_train to match the non-null entries of X_train\n",
        "y_train_filtered = y_train[X_train.notna()]\n",
        "\n",
        "modelo_svm_lineal.fit(X=tfidf_train, y=y_train_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-aJ5gQdV5E4W"
      },
      "outputs": [],
      "source": [
        "# Grid de hiperparámetros\n",
        "# ==============================================================================\n",
        "param_grid = {'C': np.logspace(-5, 3, 10)}\n",
        "\n",
        "# Búsqueda por validación cruzada\n",
        "# ==============================================================================\n",
        "grid = GridSearchCV(\n",
        "        estimator  = svm.SVC(kernel= \"linear\"),\n",
        "        param_grid = param_grid,\n",
        "        scoring    = 'accuracy',\n",
        "        n_jobs     = -1,\n",
        "        cv         = 5,\n",
        "        verbose    = 0,\n",
        "        return_train_score = True\n",
        "      )\n",
        "\n",
        "# Se asigna el resultado a _ para que no se imprima por pantalla\n",
        "_ = grid.fit(X = tfidf_train, y = y_train[X_train.notna()])\n",
        "\n",
        "# Resultados del grid\n",
        "# ==============================================================================\n",
        "resultados = pd.DataFrame(grid.cv_results_)\n",
        "resultados.filter(regex = '(param.*|mean_t|std_t)')\\\n",
        "    .drop(columns = 'params')\\\n",
        "    .sort_values('mean_test_score', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPdWpWen5LZF"
      },
      "outputs": [],
      "source": [
        "# Mejores hiperparámetros por validación cruzada\n",
        "# ==============================================================================\n",
        "print(\"----------------------------------------\")\n",
        "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
        "print(\"----------------------------------------\")\n",
        "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
        "\n",
        "modelo_final = grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DExgJcB5QEu"
      },
      "outputs": [],
      "source": [
        "# Error predicciones test\n",
        "# ==============================================================================\n",
        "predicciones_test = modelo_final.predict(X=tfidf_test)\n",
        "\n",
        "# Align y_test with the filtered X_test that was used for prediction\n",
        "y_test_filtered_for_comparison = y_test[X_test.notna()]\n",
        "\n",
        "print(\"-------------\")\n",
        "print(\"Error de test\")\n",
        "print(\"-------------\")\n",
        "\n",
        "print(f\"Número de clasificaciones erróneas de un total de {tfidf_test.shape[0]} \" \\\n",
        "      f\"clasificaciones: {(y_test_filtered_for_comparison != predicciones_test).sum()}\"\n",
        ")\n",
        "print(f\"% de error: {100*(y_test_filtered_for_comparison != predicciones_test).mean()}\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"-------------------\")\n",
        "print(\"Matriz de confusión\")\n",
        "print(\"-------------------\")\n",
        "# Get the unique classes to correctly label the confusion matrix\n",
        "classes = sorted(y_test_filtered_for_comparison.unique())\n",
        "pd.DataFrame(confusion_matrix(y_true = y_test_filtered_for_comparison, y_pred= predicciones_test),\n",
        "             columns= [f\"Predicted {c}\" for c in classes],\n",
        "             index = [f\"Actual {c}\" for c in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLx61iMV5VbG"
      },
      "outputs": [],
      "source": [
        "# Descarga lexicon sentimientos\n",
        "# ==============================================================================\n",
        "lexicon = pd.read_table(\n",
        "            'https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt',\n",
        "            names = ['termino', 'sentimiento']\n",
        "          )\n",
        "lexicon.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijdIzrJL5adB"
      },
      "outputs": [],
      "source": [
        "# Sentimiento promedio\n",
        "# ==============================================================================\n",
        "tweets_sentimientos = pd.merge(\n",
        "                            left     = tweets_tidy,\n",
        "                            right    = lexicon,\n",
        "                            left_on  = \"token\",\n",
        "                            right_on = \"termino\",\n",
        "                            how      = \"inner\"\n",
        "                      )\n",
        "\n",
        "tweets_sentimientos = tweets_sentimientos.drop(columns = \"termino\")\n",
        "\n",
        "# The dataframe 'tweets_sentimientos' now contains the numeric 'sentimiento' column from the lexicon.\n",
        "# No further sum/grouping is needed here if the intent is to analyze individual sentiments per 'edad_cat'\n",
        "# in the next cell.\n",
        "tweets_sentimientos.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76isHdsj5djt"
      },
      "outputs": [],
      "source": [
        "def perfil_sentimientos(df):\n",
        "    print(autor)\n",
        "    print(\"=\" * 12)\n",
        "    print(f\"Positivos: {round(100 * np.mean(df.sentimiento > 0), 2)}\")\n",
        "    print(f\"Neutros  : {round(100 * np.mean(df.sentimiento == 0), 2)}\")\n",
        "    print(f\"Negativos: {round(100 * np.mean(df.sentimiento < 0), 2)}\")\n",
        "    print(\" \")\n",
        "\n",
        "for autor, df in tweets_sentimientos.groupby(\"edad_cat\"):\n",
        "    perfil_sentimientos(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdPLfDcs5k6G"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "\n",
        "for edad_cat in tweets_sentimientos.edad_cat.unique():\n",
        "    df = tweets_sentimientos[tweets_sentimientos.edad_cat == edad_cat].copy()\n",
        "    # Assuming 'fecha_servicio' is the date column to use for time series analysis\n",
        "    # If 'Fecha encuesta' was intended, the column name must be consistent\n",
        "    df['fecha_servicio'] = pd.to_datetime(df['fecha_servicio'])\n",
        "    df = df.set_index(\"fecha_servicio\")\n",
        "    df = df[['sentimiento']].resample('1M').mean()\n",
        "    ax.plot(df.index, df.sentimiento, label=edad_cat)\n",
        "\n",
        "ax.set_title(\"Sentimiento promedio de los comentarios por mes\")\n",
        "ax.legend();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA49pnSJ8EcB5tpZ2r1Man",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}